{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "sentiment-analysis-twitter-and-facebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akib26/TwitterSentimentAnalysis/blob/v4/sentiment_analysis_twitter_and_facebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vla-sEtXYnbk"
      },
      "source": [
        "# Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be95aTaV3uaX",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821f57fb-05d0-4d89-edfd-13faa26f7062"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.classify import SklearnClassifier\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.metrics import accuracy\n",
        "from nltk.metrics import ConfusionMatrix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5FMyy7-3vjq",
        "trusted": true
      },
      "source": [
        "data = pd.read_csv('Sentiment1.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "# Splitting the dataset into train and test set\n",
        "X=data['text']\n",
        "y=data['sentiment']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCdaCzCz34vN",
        "trusted": true
      },
      "source": [
        "# Splitting the dataset into train and test set\n",
        "X_train,X_test,y_train, y_test = train_test_split(data,y,test_size = 0.1,random_state=1,stratify=y)\n",
        "#train,test=train_test_split(data,test_size=0.1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pseuRHPz5wav",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc504802-fc8b-45c5-b390-6c4b5fdabc51"
      },
      "source": [
        "X_train.shape,X_test.shape,y_train.shape, y_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12483, 2), (1388, 2), (12483,), (1388,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LSA5CWW32kM",
        "trusted": true
      },
      "source": [
        "def wordcloud_draw(file,data,key, color = 'black'):\n",
        "    words = ' '.join(data)\n",
        "    cleaned_word = \" \".join([word for word in words.split()\n",
        "                            if 'http' not in word\n",
        "                                and not word.startswith('@')\n",
        "                                and not word.startswith('#')\n",
        "                                and word != 'RT'\n",
        "                            ])\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                      background_color=color,\n",
        "                      width=2500,\n",
        "                      height=2000\n",
        "                     ).generate(cleaned_word)\n",
        "    plt.figure(1,figsize=(13, 13))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.title('Wordcloud of key \"{}\"'.format(key))\n",
        "    plt.savefig('{}{}.png'.format(file,key))\n",
        "    plt.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ATjQMxwR_U0y"
      },
      "source": [
        "dict_of_Categorization = {k: v for k, v in data.groupby('sentiment')}\n",
        "\n",
        "for key, value in dict_of_Categorization.items():\n",
        "    num_Descriptions = len(value)\n",
        "    if (num_Descriptions >= 50):\n",
        "        num_Descriptions\n",
        "        wordcloud_draw(\"Twitter_\",value[\"text\"], key, 'white')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wke1M8YG4N03",
        "trusted": true
      },
      "source": [
        "tweets = []\n",
        "stopwords_set = set(stopwords.words(\"english\"))\n",
        "for index,row in X_train.iterrows():\n",
        "    words_filtered = [e.lower() for e in row.text.split() if len(e) >= 3]\n",
        "    words_cleaned = [word for word in words_filtered\n",
        "        if 'http' not in word\n",
        "        and not word.startswith('@')\n",
        "        and not word.startswith('#')\n",
        "        and word != 'RT']\n",
        "    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n",
        "    tweets.append((words_without_stopwords, row.sentiment))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s8a4jy65owu",
        "trusted": true
      },
      "source": [
        "# Extracting word features\n",
        "def get_words_in_tweets(tweets):\n",
        "    all = []\n",
        "    for (words, sentiment) in tweets:\n",
        "        all.extend(words)\n",
        "    return all\n",
        " \n",
        "def get_word_features(wordlist):\n",
        "    wordlist = nltk.FreqDist(wordlist)\n",
        "    features = wordlist.keys()\n",
        "    return features\n",
        "w_features = get_word_features(get_words_in_tweets(tweets))\n",
        " \n",
        "def extract_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in w_features:\n",
        "        features['contains(%s)' % word] = (word in document_words)\n",
        "    return features"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI3GwnWl5qwD",
        "trusted": true
      },
      "source": [
        "# Training the Naive Bayes classifier\n",
        "training_set = nltk.classify.apply_features(extract_features,tweets)\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8sbYR1nSmrZ",
        "trusted": true
      },
      "source": [
        "predicted=[]\n",
        "for obj in X_test['text']:\n",
        "  res=classifier.classify(extract_features(obj.split()))\n",
        "  predicted.append(res)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUjz0sZHT226",
        "trusted": true
      },
      "source": [
        "y_test_list=y_test.tolist()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrVU2pmLN2bX",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff2b322-2638-4201-d4e1-96af6711cf5b"
      },
      "source": [
        "\n",
        "print(ConfusionMatrix(y_test_list, predicted).pretty_format(sort_by_count=True))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         |   P       N |\n",
            "         |   o   N   e |\n",
            "         |   s   e   g |\n",
            "         |   i   u   a |\n",
            "         |   t   t   t |\n",
            "         |   i   r   i |\n",
            "         |   v   a   v |\n",
            "         |   e   l   e |\n",
            "---------+-------------+\n",
            "Positive |<442> 87  22 |\n",
            " Neutral |  80<357> 11 |\n",
            "Negative | 111  65<213>|\n",
            "---------+-------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG300P7qVVPs"
      },
      "source": [
        "The numbers embedded in <> are the true positives (tp)meaning correctly classified.\n",
        "\n",
        "\n",
        "*   rows refer to actual test set sentiment\n",
        "*   columns refer to predicted sentiment by our naive bayes classifier\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF1HpM-sEQ0X",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ffd0bb-57fa-4159-e19d-2c86fc214cc6"
      },
      "source": [
        "print(\"Accuracy:\", accuracy(y_test_list, predicted))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.729106628242075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nBdetJnHbEH",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9ddbe8-dbbb-4afd-a487-694effee5a42"
      },
      "source": [
        "\n",
        "user_input=input(\"Enter your tweet\")\n",
        "res=classifier.classify(extract_features(user_input.split()))\n",
        "print(res)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your tweetthis is bad \n",
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y5FARuG8487",
        "trusted": true
      },
      "source": [
        "import pickle\n",
        "mymodel = 'naive_finalized_model.sav'\n",
        "pickle.dump(classifier, open(mymodel, 'wb'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bNdb3Ra8_U1D"
      },
      "source": [
        "with open('tweets.pkl', 'wb') as f:\n",
        "    pickle.dump(tweets, f)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yWEYIpYYjae"
      },
      "source": [
        "# Facebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mdy14udYy1Y",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db9ba39-47d6-4a9b-e3eb-cdded22d6a5d"
      },
      "source": [
        "fb = pd.read_csv('fb_sentiment.csv')\n",
        "fb = fb[['text','sentiment']]\n",
        "print(fb.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text sentiment\n",
            "0  Drug Runners and  a U.S. Senator have somethin...   Neutral\n",
            "1  Heres a single, to add, to Kindle. Just read t...   Neutral\n",
            "2  If you tire of Non-Fiction.. Check out http://...   Neutral\n",
            "3    Ghost of Round Island is supposedly nonfiction.   Neutral\n",
            "4  Why is Barnes and Nobles version of the Kindle...  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y118dVmJZxiv",
        "trusted": true
      },
      "source": [
        "fbpredicted=[]\n",
        "for obj in fb['text']:\n",
        "  res=classifier.classify(extract_features(obj.split()))\n",
        "  fbpredicted.append(res)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-OmXqOMlIeN",
        "trusted": true
      },
      "source": [
        "fbactual=fb['sentiment'].tolist()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJB-ploom1DV",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfe78cd-1446-4c26-a931-367f5094bec4"
      },
      "source": [
        "\n",
        "print(ConfusionMatrix(fbactual, fbpredicted).pretty_format(sort_by_count=True))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         |   P       N |\n",
            "         |   o   N   e |\n",
            "         |   s   e   g |\n",
            "         |   i   u   a |\n",
            "         |   t   t   t |\n",
            "         |   i   r   i |\n",
            "         |   v   a   v |\n",
            "         |   e   l   e |\n",
            "---------+-------------+\n",
            "Positive |<465>124  52 |\n",
            " Neutral |  93<156> 31 |\n",
            "Negative |  27  11 <41>|\n",
            "---------+-------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cu_camOhGfF",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a306cd9-6921-4851-ef6a-711f968ecc9b"
      },
      "source": [
        "print(\"Accuracy:\", accuracy(fbactual, fbpredicted))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Df7-utMh_U1L"
      },
      "source": [
        "dict_of_Categorization = {k: v for k, v in fb.groupby('sentiment')}\n",
        "\n",
        "for key, value in dict_of_Categorization.items():\n",
        "    num_Descriptions = len(value)\n",
        "    if (num_Descriptions >= 50):\n",
        "        num_Descriptions\n",
        "        wordcloud_draw(\"FB\",value[\"text\"], key, 'white')"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}